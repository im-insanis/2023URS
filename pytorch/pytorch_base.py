# -*- coding: utf-8 -*-
"""Pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TASRI2hdXdARyygrJpxwmaDG8sq4K_B6

#Pytorch 2-2
"""

import torch

t=torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)

print(t.dim())
print(t.shape)
print(t.size())

print(t[0], t[-1])
print(t[2:5])

t=torch.FloatTensor([[1., 2., 3], [4., 5., 6.], [7., 8., 9.], [10., 11., 12]])
print(t)

print(t.dim())
print(t.size())

print(t[:, 1])
print(t[:, 1].size())

print(t[:, :-1])

m1=torch.FloatTensor([[3, 3]])
m2=torch.FloatTensor([[2, 2]])
print(m1+m2)

m1=torch.FloatTensor([[1, 2]])
m2=torch.FloatTensor([3])
print(m1+m2)

m1=torch.FloatTensor([[1, 2]])
m2=torch.FloatTensor([[3], [4]])
print(m1+m2)

m1=torch.FloatTensor([[1, 2], [3, 4]])
m2=torch.FloatTensor([[1], [2]])
print(m1*m2)
print(m1.mul(m2))

print(m1.matmul(m2))

t = torch.FloatTensor([[1, 2], [3, 4]])
print(t.mean(dim=0))
print(t.mean(dim=1))
print(t.mean(dim=-1))

print(t.sum())
print(t.sum(dim=0))
print(t.sum(dim=1))
print(t.sum(dim=-1))

print(t.max())
print(t.max(dim=0))
print(t.max(dim=1))
print(t.max(dim=-1))

"""#Pytorch 2-3"""

import numpy as np

t=np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])
ft=torch.FloatTensor(t)
print(ft)
print(ft.shape)

print(ft.view(-1, 3))
print(ft.view(-1, 1, 3))
print(ft.view(-1, 1, 3).shape)

ft=torch.FloatTensor([[0], [1], [2]])
print(ft)
print(ft.shape)

print(ft.squeeze())
print(ft.squeeze().shape)

import torch
lt=torch.LongTensor([1, 2, 3, 4])
bt=torch.ByteTensor([True, False, True, False])
print(lt)
print(lt.float())
print(bt)
print(bt.long())
print(bt.float())

x=torch.FloatTensor([1, 4])
y=torch.FloatTensor([2, 5])
z=torch.FloatTensor([3, 6])
print(torch.stack([x, y, z]))
print(torch.stack([x, y, z], dim=1))

x=torch.IntTensor([[0, 1, 2], [3, 4, 5]])
print(torch.ones_like(x))
print(torch.zeros_like(x))

"""#3-1 선형회귀"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x_train=torch.FloatTensor([[1], [2], [3]])
y_train=torch.FloatTensor([[2], [4], [6]])

print(x_train)
print(x_train.shape)

print(y_train)
print(y_train.shape)

W=torch.zeros(1, requires_grad=True)
print(W)

b=torch.zeros(1, requires_grad=True)
print(b)

H=x_train*W+b
print(H)

C=torch.mean((H-y_train)**2)
print(C)

optimizer=optim.SGD([W, b], lr=0.01)

epoch=1999

for i in range(epoch+1):
  H=x_train*W+b

  C=torch.mean((H-y_train)**2)

  optimizer.zero_grad()
  C.backward()
  optimizer.step()

  if i%100==0:
    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(i, epoch, W.item(), b.item(), C.item()))

import torch
w=torch.tensor(2.0, requires_grad=True)
print(w)
epoch=20
for i in range(epoch+1):
  z=2*w
  z.backward()
  print(w.grad)

"""#3-2 자동 미분"""

import torch

w=torch.tensor(2.0, requires_grad=True)
y=w**2
z=2*y+5

z.backward()
print(w.grad)

"""#3-3 다중 선형 회귀"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x1_train=torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

w1=torch.zeros(1, requires_grad=True)
w2=torch.zeros(1, requires_grad=True)
w3=torch.zeros(1, requires_grad=True)
b=torch.zeros(1, requires_grad=True)

optimizer=optim.SGD([w1, w2, w3, b], lr=1e-5)
epoch=1000

for i in range(epoch+1):
  H=w1*x1_train+w2*x2_train+w3*x3_train+b
  C=torch.mean((H-y_train)**2)

  optimizer.zero_grad()
  C.backward()
  optimizer.step()

  if i%100==0:
    print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(
            i, epoch, w1.item(), w2.item(), w3.item(), b.item(), C.item()
        ))

"""#다중 선형 회귀 - 내적"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

X_train=torch.FloatTensor([[73, 80, 75], [93, 88, 93], [89, 91, 90], [96, 98, 100], [73, 66, 70]])
Y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

W=torch.zeros((3, 1), requires_grad=True)
b=torch.zeros(1, requires_grad=True)

optimizer=optim.SGD([W, b], lr=1e-5)

epoch=1000

for i in range(epoch+1):
  H=X_train.matmul(W)+b
  C=torch.mean((H-Y_train)**2)

  optimizer.zero_grad()
  C.backward()
  optimizer.step()

  if i%100==0:
    print('Epoch {:4d}/{} hypothesis: {} Cost: {}, Wgrad: {}, Bgrad: {}'.format(
        i, epoch, H.squeeze().detach(), C.item(), W, b.item()))

"""#3-4 nn.Module 선형 회귀"""

import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(1)

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

model=nn.Linear(1, 1)

print(list(model.parameters()))

optimizer=torch.optim.SGD(model.parameters(), lr=0.01)

epoch=2000

for i in range(epoch+1):
  P=model(x_train)
  C=F.mse_loss(P, y_train)

  optimizer.zero_grad()
  C.backward()
  optimizer.step()

new_var=torch.FloatTensor([[4.0]])
prd=model(new_var)
print(prd)

print(list(model.parameters()))

x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

model=nn.Linear(3, 1)
print(list(model.parameters()))

optimizer=torch.optim.SGD(model.parameters(), lr=1e-5)

nb_epochs = 2000
for epoch in range(nb_epochs+1):

    prediction = model(x_train)

    cost = F.mse_loss(prediction, y_train)

    optimizer.zero_grad()
    cost.backward()
    optimizer.step()

    if epoch % 100 == 0:
      print('Epoch {:4d}/{} Cost: {:.6f}'.format(
          epoch, nb_epochs, cost.item()
      ))

"""#3-5 클래스로 모델 구현"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class LRModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear=nn.Linear(1, 1)

  def forward(self, x):
    return self.linear(x)

class MLRModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear=nn.Linear(3, 1)

  def forward(self, x):
    return self.linear(x)

torch.manual_seed(1)

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

model=LRModel()

optimizer=optim.SGD(model.parameters(), lr=0.01)

epoch=2000

for i in range(epoch+1):
  P=model(x_train)

  C=F.mse_loss(P, y_train)

  optimizer.zero_grad()
  C.backward()
  optimizer.step()

  if i % 100 == 0:
      print('Epoch {:4d}/{} Cost: {:.6f}'.format(
          i, epoch, C.item()
      ))

"""#SoftMax 구현"""

import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(1)

z = torch.FloatTensor([1, 2, 3])

hypothesis = F.softmax(z, dim=0)
print(hypothesis)

hypothesis.sum()

z = torch.rand(3, 5, requires_grad=True)

hypothesis = F.softmax(z, dim=1)
print(hypothesis)

y = torch.randint(5, (3,)).long()
print(y)

y_one_hot = torch.zeros_like(hypothesis)
y_one_hot.scatter_(1, y.unsqueeze(1), 1)

print(y.unsqueeze(1))

"""#소프트맥스 & CrossEntropy"""

import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(1)

z=torch.FloatTensor([1, 2, 3])

H=F.softmax(z, dim=0)
print(H)
print(H.sum())

z=torch.rand(3, 5, requires_grad=True)
H=F.softmax(z, dim=1)
print(H)

y=torch.randint(5, (3,))
print(y)

y_one_hot=torch.zeros_like(H)
print(y_one_hot)

y_one_hot.scatter_(1, y.unsqueeze(1), 1)
print(y_one_hot)

cost=(y_one_hot * -torch.log(H)).sum(dim=1).mean()
print(cost)

cost=(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()
print(cost)

cost=F.nll_loss(F.log_softmax(z, dim=1), y)
print(cost)

cost=F.cross_entropy(z, y)
print(cost)

"""#CNN"""

import torch
import torch.nn as nn

inputs=torch.Tensor(1, 1, 28, 28)
print(inputs.size())

conv1=nn.Conv2d(1, 32, 3)
print(conv1)

conv2=nn.Conv2d(32, 64, 3, padding=1)
print(conv2)

pool=nn.MaxPool2d(2)
print(pool)

out=conv1(inputs)
print(out.size())

out=conv2(out)
print(out.size())

out=pool(out)
print(out.size())

out=pool(out)
print(out.size())

out=out.view(out.size(0), -1)
print(out.size())

fc=nn.Linear(out.size(1), 10)
out=fc(out)
print(out.size())